import os
import httpx
import uvicorn
import json
import re
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# --- è¨­å®š ---
OLLAMA_URL = "http://ollama:11434"
MODEL_NAME = "gemma2:9b"
EMBED_MODEL = "gemma2:9b"
DB_DIR = "./db"

app = FastAPI()

class AskRequest(BaseModel):
    question: str

embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model=EMBED_MODEL)
vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embeddings) if os.path.exists(DB_DIR) else None

def rebuild_vector_db():
    base_dir = "./knowledge"
    persist_directory = DB_DIR
    configs = {
        "short": {"chunk_size": 2000, "chunk_overlap": 300}, 
        "medium": {"chunk_size": 1000, "chunk_overlap": 200},
        "long": {"chunk_size": 1500, "chunk_overlap": 200}
    }
    all_docs = []
    for folder, config in configs.items():
        target_dir = os.path.join(base_dir, folder)
        if not os.path.exists(target_dir): continue
        print(f"ğŸ“¦ ãƒ•ã‚©ãƒ«ãƒ€èª­ã¿è¾¼ã¿ä¸­: {folder}...")
        for filename in os.listdir(target_dir):
            if filename.endswith((".md", ".txt")):
                loader = TextLoader(os.path.join(target_dir, filename), encoding='utf-8')
                raw_docs = loader.load()
                for d in raw_docs:
                    d.metadata["doc_type"] = folder
                    d.metadata["source"] = filename
                split_docs = RecursiveCharacterTextSplitter(
                    chunk_size=config["chunk_size"], chunk_overlap=config["chunk_overlap"]
                ).split_documents(raw_docs)
                all_docs.extend(split_docs)
    if all_docs:
        if os.path.exists(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
        vdb = Chroma.from_documents(documents=all_docs, embedding=embeddings, persist_directory=persist_directory)
        print(f"âœ… DBç™»éŒ²å®Œäº†: {len(all_docs)} ãƒãƒ£ãƒ³ã‚¯")
        return vdb
    return None

async def route_question_with_weights(question: str):
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€å›ç­”ã«å¿…è¦ãªã‚«ãƒ†ã‚´ãƒªã®é‡è¦åº¦(0.0-1.0)ã‚’åˆ¤å®šã—JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
- short: æ­£ç¢ºãªä¾¡æ ¼ã€é€æ–™ã€æ•°å€¤è¨ˆç®—
- medium: å‘³ã€æ·¹ã‚Œæ–¹ã€ãƒ¬ã‚·ãƒ”ã€ã‚®ãƒ•ãƒˆææ¡ˆ
è³ªå•: {question}
å›ç­”ï¼ˆJSONã®ã¿ï¼‰:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        try:
            text = resp.json().get("response", "").strip()
            return json.loads(text[text.find('{'):text.rfind('}')+1])
        except: return {"short": 0.5, "medium": 0.5, "long": 0.0}

async def generate_search_queries(question: str):
    shop_info = "è—¤å…«èŒ¶å¯®ã€‚ä¼Šå‹¢ã®æ·±è’¸ã—èŒ¶ã€ã»ã†ã˜èŒ¶ã€å’Œç´…èŒ¶ã€ä¼Šå‹¢èŒ¶ãƒ‘ã‚¦ãƒ€ãƒ¼ã€‚ãƒ‡ãƒ¼ã‚¿:ä¾¡æ ¼(short),é€æ–™(short),ãƒ¬ã‚·ãƒ”(medium)"
    prompt = f"""è—¤å…«èŒ¶å¯®ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºAIã¨ã—ã¦ã€è³ªå•ã‚’DBæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
ã€ã‚·ãƒ§ãƒƒãƒ—æƒ…å ±ã€‘:{shop_info}
å›ç­”ãƒ«ãƒ¼ãƒ«: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿å‡ºåŠ›ã€‚
è³ªå•: {question}
æ¤œç´¢ã‚¯ã‚¨ãƒª:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        return resp.json().get("response", "").strip()

@app.post("/ask")
async def ask(req: AskRequest):
    global vector_db
    if vector_db is None: return {"error": "DB not found."}

    optimized_query = await generate_search_queries(req.question)
    weights = await route_question_with_weights(req.question)

    all_results = []
    for category, weight in weights.items():
        if weight < 0.1: continue
        
        query_addon = " å•†å“å ä¾¡æ ¼ ç¨è¾¼"
        if any(x in req.question for x in ["é€æ–™", "é€ã‚‹", "é‹è³ƒ", "å±Šã‘ã‚‹"]):
            # æ¤œç´¢æ™‚ã«è³ªå•æ–‡ãã®ã‚‚ã®ã‚’æ··ãœã‚‹ã“ã¨ã§ã€åœ°åŸŸåï¼ˆæ²–ç¸„ãªã©ï¼‰ã‚’ç¢ºå®Ÿã«æ‹¾ã‚ã›ã‚‹
            query_addon = f" é€æ–™ãƒã‚¤ãƒ³ãƒˆ å…¨å›½ä¸€å¾‹280å†† é€æ–™è¨ˆç®—ãƒ«ãƒ¼ãƒ« åœ°åŸŸåˆ¥é€æ–™è¡¨ {req.question}"
            
        combined_query = f"{optimized_query} {query_addon}"
        res = vector_db.max_marginal_relevance_search(
            combined_query, k=12, fetch_k=50, filter={"doc_type": category}
        )
        all_results.extend(res)

    all_results.sort(key=lambda x: 0 if "products" in x.metadata.get("source", "") else 1)

    context_parts = []
    for i, doc in enumerate(all_results):
        clean_content = re.sub(r'\[cite.*?\]', '', doc.page_content)
        source = doc.metadata.get('source', 'unknown')
        ctype = doc.metadata.get('doc_type', 'unknown')
        context_parts.append(f"å‡ºå…¸:{source} (ã‚«ãƒ†ã‚´ãƒª:{ctype})\nå†…å®¹:{clean_content}")

    context = "\n\n".join(context_parts)

    # 5. ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ­ã‚¸ãƒƒã‚¯è§£æ±ºï¼ˆå†…éƒ¨è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆï¼‰
    logic_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®æ­£ç¢ºãªæ¥å®¢æ‹…å½“ã§ã™ã€‚
æä¾›ã•ã‚ŒãŸã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‚’å…ƒã«ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’ã€Œä¸€æ­©ãšã¤æ›¸ãå‡ºã—ã¦ã€é€æ–™ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘:
{context}

è³ªå•: {req.question}

ã€é€æ–™è¨ˆç®—ã®é‰„å‰‡ã€‘:
1. **å•†å“ã®æŠ½å‡º**: è³ªå•ã«ã‚ã‚‹å…¨ã¦ã®å•†å“ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã€ãã‚Œãã‚Œã®ã€Œé€æ–™ãƒã‚¤ãƒ³ãƒˆã€ã‚’ã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‹ã‚‰ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
2. **ãƒã‚¤ãƒ³ãƒˆåˆç®—**: (å•†å“Aã®ãƒã‚¤ãƒ³ãƒˆ Ã— å€‹æ•°) + (å•†å“Bã®ãƒã‚¤ãƒ³ãƒˆ Ã— å€‹æ•°) ... = åˆè¨ˆãƒã‚¤ãƒ³ãƒˆ ã‚’ç®—æ•°ã¨ã—ã¦è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚
3. **æ¡ä»¶åˆ†å²**:
   - åˆè¨ˆãŒ **6.0ãƒã‚¤ãƒ³ãƒˆä»¥ä¸‹** ãªã‚‰ã€å…¨å›½ä¸€å¾‹ **280å††**ã€‚
   - åˆè¨ˆãŒ **6.0ãƒã‚¤ãƒ³ãƒˆã‚’è¶…ãˆã‚‹** å ´åˆã®ã¿ã€é…é€åœ°åŸŸï¼ˆä¾‹ï¼šæ²–ç¸„çœŒï¼‰ã®é€æ–™ã‚’é©ç”¨ã€‚
4. **é€æ–™ç„¡æ–™**: åˆè¨ˆ20,000å††ä»¥ä¸Šãªã‚‰0å††ã€‚

å›ç­”ï¼ˆè¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’å…¨ã¦æ›¸ãã“ã¨ï¼‰:"""
    
    async with httpx.AsyncClient(timeout=None) as client:
        logic_resp = await client.post(
            f"{OLLAMA_URL}/api/generate",
            json={"model": MODEL_NAME, "prompt": logic_prompt, "stream": False, "options": {"temperature": 0.0}}
        )
        raw_answer = logic_resp.json().get("response")

        # 6. ã‚¹ãƒ†ãƒƒãƒ—4: æ•´å½¢ãƒ»æ¤œè¨¼ï¼ˆæ¥å®¢ç”¨æ¸…æ›¸ï¼‰
        clean_up_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®çœ‹æ¿ã‚¹ã‚¿ãƒƒãƒ•ã€ŒèŒ¶ã€…ä¸¸ã€ã§ã™ã€‚
ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘ã®æ•°å€¤ã‚’ã€ãƒ‡ãƒ¼ã‚¿ã€‘ã¨ç…§ã‚‰ã—åˆã‚ã›ã€ãŠå®¢æ§˜ã®è³ªå•ã«ã ã‘ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘:
{context}
ã€å…ƒã®è³ªå•ã€‘:
{req.question}
ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘:
{raw_answer}

æ¸…æ›¸ãƒ«ãƒ¼ãƒ«:
1. **è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆStep Xã€ãƒã‚¤ãƒ³ãƒˆã®åˆç®—å¼ãªã©ï¼‰ã¯å›ç­”ã«ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã€‚**
2. è³ªå•ãŒé€æ–™ã«ã¤ã„ã¦ãªã‚‰ã€åˆè¨ˆãƒã‚¤ãƒ³ãƒˆãŒ6.0ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ã‚’ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘ã‹ã‚‰èª­ã¿å–ã‚Šã€æ­£ç¢ºãªé‡‘é¡ï¼ˆ280å††ã‹åœ°åŸŸåˆ¥ã‹ï¼‰ã‚’æ¡ˆå†…ã™ã‚‹ã€‚
3. 6.0ptè¶…ã§åœ°åŸŸåˆ¥é€æ–™ã«ãªã‚‹å ´åˆã¯ã€Œè¦å®šã®ã‚µã‚¤ã‚ºã‚’è¶…ãˆã‚‹ãŸã‚ã€ã¨ç†ç”±ã‚’æ·»ãˆã‚‹ã€‚
4. ä¾¡æ ¼ã ã‘ã‚’èã‹ã‚ŒãŸå ´åˆã¯ã€é€æ–™ã®è©±ã¯ã—ãªã„ã€‚
5. è¦ªèº«ã§æ¸©ã‹ã„ãƒˆãƒ¼ãƒ³ã§å›ç­”ã‚’å®Œçµã•ã›ã‚‹ã€‚

å›ç­”ï¼ˆæ¥å®¢ç”¨æ¸…æ›¸ï¼‰:"""

        final_resp = await client.post(
            f"{OLLAMA_URL}/api/generate",
            json={"model": MODEL_NAME, "prompt": clean_up_prompt, "stream": False, "options": {"temperature": 0.0}}
        )
        
        return {
            "answer": final_resp.json().get("response"),
            "weights": weights,
            "debug_raw_logic": raw_answer # ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”¨ã«ä¸€æ—¦æˆ»ã—ã¦ã„ã¾ã™
        }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
