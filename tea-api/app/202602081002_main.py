import os
import httpx
import uvicorn
import json
import re
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# --- è¨­å®š ---
OLLAMA_URL = "http://ollama:11434"
MODEL_NAME = "gemma2:9b"
EMBED_MODEL = "gemma2:9b"
DB_DIR = "./db"

app = FastAPI()

class AskRequest(BaseModel):
    question: str

embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model=EMBED_MODEL)
vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embeddings) if os.path.exists(DB_DIR) else None

def rebuild_vector_db():
    base_dir = "./knowledge"
    persist_directory = DB_DIR
    configs = {
        "short": {"chunk_size": 2000, "chunk_overlap": 300}, 
        "medium": {"chunk_size": 1000, "chunk_overlap": 200},
        "long": {"chunk_size": 1500, "chunk_overlap": 200}
    }
    all_docs = []
    for folder, config in configs.items():
        target_dir = os.path.join(base_dir, folder)
        if not os.path.exists(target_dir): continue
        print(f"ğŸ“¦ ãƒ•ã‚©ãƒ«ãƒ€èª­ã¿è¾¼ã¿ä¸­: {folder}...")
        for filename in os.listdir(target_dir):
            if filename.endswith((".md", ".txt")):
                loader = TextLoader(os.path.join(target_dir, filename), encoding='utf-8')
                raw_docs = loader.load()
                for d in raw_docs:
                    d.metadata["doc_type"] = folder
                    d.metadata["source"] = filename
                split_docs = RecursiveCharacterTextSplitter(
                    chunk_size=config["chunk_size"], chunk_overlap=config["chunk_overlap"]
                ).split_documents(raw_docs)
                all_docs.extend(split_docs)
    if all_docs:
        if os.path.exists(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
        vdb = Chroma.from_documents(documents=all_docs, embedding=embeddings, persist_directory=persist_directory)
        print(f"âœ… DBç™»éŒ²å®Œäº†: {len(all_docs)} ãƒãƒ£ãƒ³ã‚¯")
        return vdb
    return None

async def route_question_with_weights(question: str):
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€å›ç­”ã«å¿…è¦ãªã‚«ãƒ†ã‚´ãƒªã®é‡è¦åº¦(0.0-1.0)ã‚’åˆ¤å®šã—JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
- short: æ­£ç¢ºãªä¾¡æ ¼ã€é€æ–™ã€æ•°å€¤è¨ˆç®—
- medium: å‘³ã€æ·¹ã‚Œæ–¹ã€ãƒ¬ã‚·ãƒ”ã€ã‚®ãƒ•ãƒˆææ¡ˆã€æ­´å²
è³ªå•: {question}
å›ç­”ï¼ˆJSONã®ã¿ï¼‰:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        try:
            text = resp.json().get("response", "").strip()
            return json.loads(text[text.find('{'):text.rfind('}')+1])
        except: return {"short": 0.5, "medium": 0.5, "long": 0.0}

async def generate_search_queries(question: str):
    shop_info = "è—¤å…«èŒ¶å¯®ã€‚ä¼Šå‹¢èŒ¶å°‚é–€åº—ã€‚æ·±è’¸ã—èŒ¶ã€ã»ã†ã˜èŒ¶ã€å’Œç´…èŒ¶ã€ãƒ‘ã‚¦ãƒ€ãƒ¼ã€ãƒ†ã‚£ãƒ¼ãƒãƒƒã‚°ã€‚"
    prompt = f"""è—¤å…«èŒ¶å¯®ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºAIã¨ã—ã¦ã€è³ªå•ã‚’DBæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
ã€ã‚·ãƒ§ãƒƒãƒ—æƒ…å ±ã€‘:{shop_info}
è³ªå•: {question}
æ¤œç´¢ã‚¯ã‚¨ãƒª:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        return resp.json().get("response", "").strip()

@app.post("/ask")
async def ask(req: AskRequest):
    global vector_db
    if vector_db is None: return {"error": "DB not found."}

    optimized_query = await generate_search_queries(req.question)
    weights = await route_question_with_weights(req.question)

    all_results = []
    for category, weight in weights.items():
        if weight < 0.1: continue
        query_addon = " å•†å“å ä¾¡æ ¼ ç¨è¾¼ å…¨ç¨®é¡ ä¸€è¦§"
        if any(x in req.question for x in ["é€æ–™", "é€ã‚‹", "é‹è³ƒ", "å±Šã‘ã‚‹"]):
            query_addon = f" é€æ–™ãƒã‚¤ãƒ³ãƒˆ 6.0ptåˆ¤å®š åœ°åŸŸåˆ¥é€æ–™è¡¨ {req.question}"
        combined_query = f"{optimized_query} {query_addon}"
        res = vector_db.max_marginal_relevance_search(
            combined_query, k=15, fetch_k=50, filter={"doc_type": category}
        )
        all_results.extend(res)

    all_results.sort(key=lambda x: 0 if "products" in x.metadata.get("source", "") else 1)

    context_parts = []
    for doc in all_results:
        clean_content = re.sub(r'\[cite.*?\]', '', doc.page_content)
        source = doc.metadata.get('source', 'unknown')
        context_parts.append(f"å‡ºå…¸:{source}\nå†…å®¹:{clean_content}")

    context = "\n\n".join(context_parts)

    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ­ã‚¸ãƒƒã‚¯è§£æ±ºï¼ˆæŠ½å‡ºã¨åˆ¤å®šã®åˆ†é›¢ï¼‰
    logic_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»æ¨è«–æ‹…å½“ã§ã™ã€‚
æä¾›ã•ã‚ŒãŸã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‚’å®¢è¦³çš„ã«åˆ†æã—ã€ä»¥ä¸‹ã®ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã€‘ã«å¾“ã£ã¦å›ç­”æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘:
{context}
è³ªå•: {req.question}

ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã€‘:
1. **æ„å›³åˆ¤å®š**: è³ªå•ã¯ã€Œäº‹å®Ÿç¢ºèªï¼ˆå•†å“ä¸€è¦§ã€ä¾¡æ ¼ã€ç‰¹å¾´ãªã©ï¼‰ã€ã‹ã€Œé€æ–™è¨ˆç®—ã€ã‹ï¼Ÿ
2. **Extractorï¼ˆäº‹å®Ÿç¢ºèªï¼‰**: 
   - è³ªå•ã«é–¢é€£ã™ã‚‹å•†å“åã‚’ã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‹ã‚‰æ­£ç¢ºã«ã€ŒæŠ½å‡ºã€ã—ã¦ãã ã•ã„ã€‚
   - ã€ç¦æ­¢ã€‘: ãƒ‡ãƒ¼ã‚¿ã«ãªã„å•†å“åï¼ˆç„™ç…èŒ¶ãªã©ï¼‰ã‚„æ¶ç©ºã®ä¾¡æ ¼ã‚’æé€ ã™ã‚‹ã“ã¨ã¯ã€Œé‡å¤§ãªè¦ç´„é•åã€ã§ã™ã€‚
   - ã€å¿…é ˆã€‘: ãƒ†ã‚£ãƒ¼ãƒãƒƒã‚°ã‚„ãƒ‘ã‚¦ãƒ€ãƒ¼ãªã©ã€å½¢çŠ¶ï¼ˆé¢¨è¢‹ï¼‰ãŒç•°ãªã‚‹ã‚‚ã®ã‚‚å…¨ã¦å€‹åˆ¥ã«ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚
3. **Reasonerï¼ˆé€æ–™è¨ˆç®—ï¼‰**:
   - é€æ–™è¨ˆç®—ãŒå¿…è¦ãªå ´åˆã®ã¿ã€å•†å“åãƒ»æ•°é‡ãƒ»é…é€å…ˆã‚’ç¢ºèªã—ã€ã€é€æ–™è¨ˆç®—ã®é‰„å‰‡ã€‘ã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚

ã€é€æ–™è¨ˆç®—ã®é‰„å‰‡ã€‘:
1. å•†å“ãƒã‚¤ãƒ³ãƒˆç‰¹å®šã€‚ 2. (ãƒã‚¤ãƒ³ãƒˆ Ã— æ³¨æ–‡æ•°) ã®åˆè¨ˆç®—å‡ºã€‚ 3. 6.0ptåˆ¤å®šï¼ˆ280å†† or åœ°åŸŸåˆ¥ï¼‰ã€‚ 4. 20,000å††ä»¥ä¸Šç„¡æ–™ã€‚

å›ç­”ï¼ˆæŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã‚’æ­£ç¢ºã«è¨˜è¿°ï¼‰:"""
    
    async with httpx.AsyncClient(timeout=None) as client:
        logic_resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": logic_prompt, "stream": False, "options": {"temperature": 0.0}})
        raw_answer = logic_resp.json().get("response")

        # ã‚¹ãƒ†ãƒƒãƒ—4: æ•´å½¢ãƒ»æ¤œè¨¼ï¼ˆæƒ…å ±ç¶­æŒã®å¾¹åº•ï¼‰
        clean_up_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®çœ‹æ¿ã‚¹ã‚¿ãƒƒãƒ•ã€ŒèŒ¶ã€…ä¸¸ã€ã§ã™ã€‚
ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘ã®æƒ…å ±ã‚’ã€è¦ªã—ã¿ã‚„ã™ã„æ¥å®¢æ–‡ã«æ•´ãˆã¦ãã ã•ã„ã€‚

ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘:
{raw_answer}

ã€æ¸…æ›¸ãƒ«ãƒ¼ãƒ«ã€‘:
1. **æƒ…å ±ã®å®Œå…¨ç¶­æŒï¼ˆæœ€é‡è¦ï¼‰**: ãƒ‰ãƒ©ãƒ•ãƒˆã«ã‚ã‚‹ã€Œå…·ä½“çš„ãªå•†å“åã€ã‚„ã€Œä¾¡æ ¼ã€ã¯ã€è¦ç´„ã—ãŸã‚Šå‰Šã£ãŸã‚Šã›ãšã€å¿…ãšå…¨ã¦å›ç­”ã«å«ã‚ã¦ãã ã•ã„ã€‚ã€Œè±Šå¯Œã§ã™ã­ã€ã¨ä¸€è¨€ã§ã¾ã¨ã‚ã‚‹ã®ã¯ã€Œç¦æ­¢ã€ã§ã™ã€‚
2. **ãƒ‡ãƒ¼ã‚¿ã®è£ä»˜ã‘**: ãƒ‰ãƒ©ãƒ•ãƒˆã®å†…å®¹ãŒã€ãƒ‡ãƒ¼ã‚¿ã€‘ã¨çŸ›ç›¾ã—ã¦ã„ãªã„ã‹æœ€çµ‚ç¢ºèªã—ã€çŸ›ç›¾ãŒã‚ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿ã€‘ã®æ•°å€¤ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
3. è¨ˆç®—å¼ã‚„ãƒã‚¤ãƒ³ãƒˆæ•°ãªã©ã®å†…éƒ¨ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€æ¥å®¢ã«ä¸è¦ãªãŸã‚å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚
4. è³ªå•ã«é€æ–™ãŒå«ã¾ã‚Œãªã„å ´åˆã¯ã€é€æ–™ã®è©±é¡Œã¯å‡ºã•ãªã„ã§ãã ã•ã„ã€‚

å›ç­”ï¼ˆæ¥å®¢ç”¨æ¸…æ›¸ï¼‰:"""

        final_resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": clean_up_prompt, "stream": False, "options": {"temperature": 0.0}})
        
        return {
            "answer": final_resp.json().get("response"),
            "weights": weights,
            "debug_raw_logic": raw_answer
        }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
