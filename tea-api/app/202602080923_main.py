import os
import httpx
import uvicorn
import json
import re
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# --- è¨­å®š ---
OLLAMA_URL = "http://ollama:11434"
MODEL_NAME = "gemma2:9b"
EMBED_MODEL = "gemma2:9b"
DB_DIR = "./db"

app = FastAPI()

class AskRequest(BaseModel):
    question: str

embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model=EMBED_MODEL)
vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embeddings) if os.path.exists(DB_DIR) else None

def rebuild_vector_db():
    base_dir = "./knowledge"
    persist_directory = DB_DIR
    configs = {
        "short": {"chunk_size": 2000, "chunk_overlap": 300}, 
        "medium": {"chunk_size": 1000, "chunk_overlap": 200},
        "long": {"chunk_size": 1500, "chunk_overlap": 200}
    }
    all_docs = []
    for folder, config in configs.items():
        target_dir = os.path.join(base_dir, folder)
        if not os.path.exists(target_dir): continue
        print(f"ğŸ“¦ ãƒ•ã‚©ãƒ«ãƒ€èª­ã¿è¾¼ã¿ä¸­: {folder}...")
        for filename in os.listdir(target_dir):
            if filename.endswith((".md", ".txt")):
                loader = TextLoader(os.path.join(target_dir, filename), encoding='utf-8')
                raw_docs = loader.load()
                for d in raw_docs:
                    d.metadata["doc_type"] = folder
                    d.metadata["source"] = filename
                split_docs = RecursiveCharacterTextSplitter(
                    chunk_size=config["chunk_size"], chunk_overlap=config["chunk_overlap"]
                ).split_documents(raw_docs)
                all_docs.extend(split_docs)
    if all_docs:
        if os.path.exists(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
        vdb = Chroma.from_documents(documents=all_docs, embedding=embeddings, persist_directory=persist_directory)
        print(f"âœ… DBç™»éŒ²å®Œäº†: {len(all_docs)} ãƒãƒ£ãƒ³ã‚¯")
        return vdb
    return None

async def route_question_with_weights(question: str):
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€å›ç­”ã«å¿…è¦ãªã‚«ãƒ†ã‚´ãƒªã®é‡è¦åº¦(0.0-1.0)ã‚’åˆ¤å®šã—JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
- short: æ­£ç¢ºãªä¾¡æ ¼ã€é€æ–™ã€æ•°å€¤è¨ˆç®—
- medium: å‘³ã€æ·¹ã‚Œæ–¹ã€ãƒ¬ã‚·ãƒ”ã€ã‚®ãƒ•ãƒˆææ¡ˆ
è³ªå•: {question}
å›ç­”ï¼ˆJSONã®ã¿ï¼‰:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        try:
            text = resp.json().get("response", "").strip()
            return json.loads(text[text.find('{'):text.rfind('}')+1])
        except: return {"short": 0.5, "medium": 0.5, "long": 0.0}

async def generate_search_queries(question: str):
    shop_info = "è—¤å…«èŒ¶å¯®ã€‚ä¼Šå‹¢ã®æ·±è’¸ã—èŒ¶ã€ã»ã†ã˜èŒ¶ã€å’Œç´…èŒ¶ã€ä¼Šå‹¢èŒ¶ãƒ‘ã‚¦ãƒ€ãƒ¼ã€‚ãƒ‡ãƒ¼ã‚¿:ä¾¡æ ¼(short),é€æ–™(short),ãƒ¬ã‚·ãƒ”(medium)"
    prompt = f"""è—¤å…«èŒ¶å¯®ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºAIã¨ã—ã¦ã€è³ªå•ã‚’DBæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
ã€ã‚·ãƒ§ãƒƒãƒ—æƒ…å ±ã€‘:{shop_info}
è³ªå•: {question}
æ¤œç´¢ã‚¯ã‚¨ãƒª:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        return resp.json().get("response", "").strip()

@app.post("/ask")
async def ask(req: AskRequest):
    global vector_db
    if vector_db is None: return {"error": "DB not found."}

    optimized_query = await generate_search_queries(req.question)
    weights = await route_question_with_weights(req.question)

    all_results = []
    for category, weight in weights.items():
        if weight < 0.1: continue

        query_addon = " å•†å“å å…¨ç¨®é¡ ä¸€è¦§ ä¾¡æ ¼ ç¨è¾¼"
        if any(x in req.question for x in ["é€æ–™", "é€ã‚‹", "é‹è³ƒ", "å±Šã‘ã‚‹"]):
            query_addon = f" é€æ–™ãƒã‚¤ãƒ³ãƒˆ å…¨å›½ä¸€å¾‹280å†† é€æ–™è¨ˆç®—ãƒ«ãƒ¼ãƒ« åœ°åŸŸåˆ¥é€æ–™è¡¨ {req.question}"

        combined_query = f"{optimized_query} {query_addon}"
        res = vector_db.max_marginal_relevance_search(
            combined_query, k=15, fetch_k=50, filter={"doc_type": category}
        )
        all_results.extend(res)

    all_results.sort(key=lambda x: 0 if "products" in x.metadata.get("source", "") else 1)

    context_parts = []
    for doc in all_results:
        clean_content = re.sub(r'\[cite.*?\]', '', doc.page_content)
        source = doc.metadata.get('source', 'unknown')
        ctype = doc.metadata.get('doc_type', 'unknown')
        context_parts.append(f"å‡ºå…¸:{source} (ã‚«ãƒ†ã‚´ãƒª:{ctype})\nå†…å®¹:{clean_content}")

    context = "\n\n".join(context_parts)

    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ­ã‚¸ãƒƒã‚¯è§£æ±ºï¼ˆå†…éƒ¨è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆï¼‰
    logic_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®æ­£ç¢ºãªæ¥å®¢æ‹…å½“ã§ã™ã€‚
ã¾ãšè³ªå•ã®ã€æ„å›³ã€‘ã‚’åˆ†æã—ã€ãã®å¾Œã§é©åˆ‡ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æä¾›ã•ã‚ŒãŸã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‚’å…ƒã«ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’ã€Œä¸€æ­©ãšã¤ç®—æ•°ã¨ã—ã¦æ›¸ãå‡ºã—ã¦ã€é€æ–™ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘:
{context}

è³ªå•: {req.question}

ã€æ€è€ƒãƒ»å›ç­”ãƒ—ãƒ­ã‚»ã‚¹ã€‘:
1. **æ„å›³åˆ¤å®š**: è³ªå•ã¯ã€Œäº‹å®Ÿç¢ºèªï¼ˆå•†å“ä¸€è¦§ã€ä¾¡æ ¼ã€ç‰¹å¾´ã€ãƒ¬ã‚·ãƒ”ãªã©ï¼‰ã€ã‹ã€Œé€æ–™ã®è¨ˆç®—ã€ã‹ï¼Ÿ
2. **äº‹å®Ÿç¢ºèªã®å ´åˆ**:
   - **ç¶²ç¾…æ€§ã®ç¢ºä¿**: è³ªå•ã«é–¢é€£ã™ã‚‹å•†å“ã‚’ã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‹ã‚‰ã€Œå…¨ã¦ã€è¦‹ã¤ã‘å‡ºã—ã¦ãã ã•ã„ã€‚
   - **å•†å“ç¨®é¡ã¸ã®å›ç­”**: èŒ¶è‘‰ï¼ˆé€šå¸¸ã®è¢‹ï¼‰ã€ãƒ†ã‚£ãƒ¼ãƒãƒƒã‚°ã€ãƒ‘ã‚¦ãƒ€ãƒ¼ãªã©ã€**ã€Œå½¢çŠ¶ï¼ˆé¢¨è¢‹ï¼‰ãŒç•°ãªã‚‹ã‚‚ã®ã€ã€ãƒ†ã‚£ãƒãƒƒã‚°ã®å€‹æ•°ã€å®¹é‡ãŒé•ã†ã‚‚ã®ã‚‚åˆ¥ã®å•†å“ã¨ã—ã¦å…¨ã¦åˆ—æŒ™**ã™ã‚‹ã“ã¨ã€‚
   - ã€Œ2ç¨®é¡ã‚ã‚Šã¾ã™ã€ã®ã‚ˆã†ã«æ•°ã‚’é™å®šã›ãšã€è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’å…¨ã¦æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚
   - ä¾¡æ ¼ãŒãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚‹å ´åˆã¯å¿…ãšä½µè¨˜ã—ã¦ãã ã•ã„ã€‚
3. **é€æ–™è¨ˆç®—ã®å ´åˆ**:
   - ã€é€æ–™è¨ˆç®—ã®é‰„å‰‡ã€‘ã«å¾“ã„ã€è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚
   - æƒ…å ±ä¸è¶³ãªã‚‰å„ªã—ãèãè¿”ã—ã¦ãã ã•ã„ã€‚

ã€é€æ–™è¨ˆç®—ã®é‰„å‰‡ã€‘:
1. **å•†å“ã®ãƒã‚¤ãƒ³ãƒˆç‰¹å®š**: å•†å“1è¢‹ã‚ãŸã‚Šã®ãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®šã€‚
2. **åˆç®—å¼ã®ä½œæˆ**: (ãƒã‚¤ãƒ³ãƒˆ Ã— æ³¨æ–‡è¢‹æ•°) ã®åˆè¨ˆã‚’ç®—å‡ºã€‚
3. **åˆ¤å®š**: 6.0ptä»¥ä¸‹ãªã‚‰280å††ã€è¶…ãˆã‚‹å ´åˆã¯æŒ‡å®šåœ°åŸŸã®é€æ–™ã‚’é©ç”¨ã€‚
4. **é€æ–™ç„¡æ–™**: ç¨è¾¼åˆè¨ˆ20,000å††ä»¥ä¸Šãªã‚‰0å††ã€‚

å›ç­”ï¼ˆäº‹å®Ÿç¢ºèªãªã‚‰å³ç­”ã€é€æ–™è¨ˆç®—ãªã‚‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¨˜è¿°ï¼‰:"""

    async with httpx.AsyncClient(timeout=None) as client:
        logic_resp = await client.post(
            f"{OLLAMA_URL}/api/generate",
            json={"model": MODEL_NAME, "prompt": logic_prompt, "stream": False, "options": {"temperature": 0.0}}
        )
        raw_answer = logic_resp.json().get("response")

        # ã‚¹ãƒ†ãƒƒãƒ—4: æ•´å½¢ãƒ»æ¤œè¨¼ï¼ˆæ¥å®¢ç”¨æ¸…æ›¸ï¼‰
        clean_up_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®çœ‹æ¿ã‚¹ã‚¿ãƒƒãƒ•ã€ŒèŒ¶ã€…ä¸¸ã€ã§ã™ã€‚
ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘ã‚’å…ƒã«ã€ãŠå®¢æ§˜ã®è³ªå•ã«ã ã‘ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘:
{context}
ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘:
{raw_answer}

æ¸…æ›¸ãƒ«ãƒ¼ãƒ«:
1. **å›ç­”æœ¬æ–‡ã«ã¯è¨ˆç®—å¼ã‚„ãƒã‚¤ãƒ³ãƒˆã®æ•°å€¤ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã€‚**
2. çµè«–ï¼ˆé‡‘é¡ï¼‰ãŒãƒ‡ãƒ¼ã‚¿ã®ãƒ«ãƒ¼ãƒ«ã¨åˆã£ã¦ã„ã‚‹ã‹ã€ãƒ‰ãƒ©ãƒ•ãƒˆã‚’éä¿¡ã›ãšå†ç¢ºèªã™ã‚‹ã€‚
3. è³ªå•ã«é€æ–™ãŒå«ã¾ã‚Œãªã„å ´åˆã¯ã€é€æ–™ã®è©±ã¯ä¸€åˆ‡ã—ãªã„ã€‚
4. èŒ¶ã€…ä¸¸ã¨ã—ã¦æ¸©ã‹ã„ãƒˆãƒ¼ãƒ³ã§å›ç­”ã™ã‚‹ã€‚

å›ç­”ï¼ˆæ¥å®¢ç”¨æ¸…æ›¸ï¼‰:"""

        final_resp = await client.post(
            f"{OLLAMA_URL}/api/generate",
            json={"model": MODEL_NAME, "prompt": clean_up_prompt, "stream": False, "options": {"temperature": 0.0}}
        )
        
        return {
            "answer": final_resp.json().get("response"),
            "weights": weights,
            "debug_raw_logic": raw_answer  # æ¤œè¨¼ç”¨ã«æ®‹ã—ã¦ã„ã¾ã™
        }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
