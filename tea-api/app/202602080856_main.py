import os
import httpx
import uvicorn
import json
import re
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# --- è¨­å®š ---
OLLAMA_URL = "http://ollama:11434"
MODEL_NAME = "gemma2:9b"
EMBED_MODEL = "gemma2:9b"
DB_DIR = "./db"

app = FastAPI()

class AskRequest(BaseModel):
    question: str

embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model=EMBED_MODEL)
vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embeddings) if os.path.exists(DB_DIR) else None

def rebuild_vector_db():
    base_dir = "./knowledge"
    persist_directory = DB_DIR
    configs = {
        "short": {"chunk_size": 2000, "chunk_overlap": 300}, 
        "medium": {"chunk_size": 1000, "chunk_overlap": 200},
        "long": {"chunk_size": 1500, "chunk_overlap": 200}
    }
    all_docs = []
    for folder, config in configs.items():
        target_dir = os.path.join(base_dir, folder)
        if not os.path.exists(target_dir): continue
        print(f"ğŸ“¦ ãƒ•ã‚©ãƒ«ãƒ€èª­ã¿è¾¼ã¿ä¸­: {folder}...")
        for filename in os.listdir(target_dir):
            if filename.endswith((".md", ".txt")):
                loader = TextLoader(os.path.join(target_dir, filename), encoding='utf-8')
                raw_docs = loader.load()
                for d in raw_docs:
                    d.metadata["doc_type"] = folder
                    d.metadata["source"] = filename
                split_docs = RecursiveCharacterTextSplitter(
                    chunk_size=config["chunk_size"], chunk_overlap=config["chunk_overlap"]
                ).split_documents(raw_docs)
                all_docs.extend(split_docs)
    if all_docs:
        if os.path.exists(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
        vdb = Chroma.from_documents(documents=all_docs, embedding=embeddings, persist_directory=persist_directory)
        print(f"âœ… DBç™»éŒ²å®Œäº†: {len(all_docs)} ãƒãƒ£ãƒ³ã‚¯")
        return vdb
    return None

async def route_question_with_weights(question: str):
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€å›ç­”ã«å¿…è¦ãªã‚«ãƒ†ã‚´ãƒªã®é‡è¦åº¦(0.0-1.0)ã‚’åˆ¤å®šã—JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
- short: æ­£ç¢ºãªä¾¡æ ¼ã€é€æ–™ã€æ•°å€¤è¨ˆç®—
- medium: å‘³ã€æ·¹ã‚Œæ–¹ã€ãƒ¬ã‚·ãƒ”ã€ã‚®ãƒ•ãƒˆææ¡ˆ
è³ªå•: {question}
å›ç­”ï¼ˆJSONã®ã¿ï¼‰:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        try:
            text = resp.json().get("response", "").strip()
            return json.loads(text[text.find('{'):text.rfind('}')+1])
        except: return {"short": 0.5, "medium": 0.5, "long": 0.0}

async def generate_search_queries(question: str):
    shop_info = "è—¤å…«èŒ¶å¯®ã€‚ä¼Šå‹¢ã®æ·±è’¸ã—èŒ¶ã€ã»ã†ã˜èŒ¶ã€å’Œç´…èŒ¶ã€ä¼Šå‹¢èŒ¶ãƒ‘ã‚¦ãƒ€ãƒ¼ã€‚ãƒ‡ãƒ¼ã‚¿:ä¾¡æ ¼(short),é€æ–™(short),ãƒ¬ã‚·ãƒ”(medium)"
    prompt = f"""è—¤å…«èŒ¶å¯®ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºAIã¨ã—ã¦ã€è³ªå•ã‚’DBæ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
ã€ã‚·ãƒ§ãƒƒãƒ—æƒ…å ±ã€‘:{shop_info}
è³ªå•: {question}
æ¤œç´¢ã‚¯ã‚¨ãƒª:"""
    async with httpx.AsyncClient(timeout=None) as client:
        resp = await client.post(f"{OLLAMA_URL}/api/generate", json={"model": MODEL_NAME, "prompt": prompt, "stream": False, "options": {"temperature": 0.0}})
        return resp.json().get("response", "").strip()

@app.post("/ask")
async def ask(req: AskRequest):
    global vector_db
    if vector_db is None: return {"error": "DB not found."}

    optimized_query = await generate_search_queries(req.question)
    weights = await route_question_with_weights(req.question)

    all_results = []
    for category, weight in weights.items():
        if weight < 0.1: continue
        query_addon = " å•†å“å ä¾¡æ ¼ ç¨è¾¼"
        if any(x in req.question for x in ["é€æ–™", "é€ã‚‹", "é‹è³ƒ", "å±Šã‘ã‚‹"]):
            query_addon = f" é€æ–™ãƒã‚¤ãƒ³ãƒˆ å…¨å›½ä¸€å¾‹280å†† é€æ–™è¨ˆç®—ãƒ«ãƒ¼ãƒ« åœ°åŸŸåˆ¥é€æ–™è¡¨ {req.question}"
        combined_query = f"{optimized_query} {query_addon}"
        res = vector_db.max_marginal_relevance_search(
            combined_query, k=12, fetch_k=50, filter={"doc_type": category}
        )
        all_results.extend(res)

    all_results.sort(key=lambda x: 0 if "products" in x.metadata.get("source", "") else 1)

    context_parts = []
    for doc in all_results:
        clean_content = re.sub(r'\[cite.*?\]', '', doc.page_content)
        source = doc.metadata.get('source', 'unknown')
        ctype = doc.metadata.get('doc_type', 'unknown')
        context_parts.append(f"å‡ºå…¸:{source} (ã‚«ãƒ†ã‚´ãƒª:{ctype})\nå†…å®¹:{clean_content}")

    context = "\n\n".join(context_parts)

    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ­ã‚¸ãƒƒã‚¯è§£æ±ºï¼ˆå†…éƒ¨è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆï¼‰
    logic_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®æ­£ç¢ºãªæ¥å®¢æ‹…å½“ã§ã™ã€‚
ã¾ãšè³ªå•ã®ã€æ„å›³ã€‘ã‚’åˆ†æã—ã€ãã®å¾Œã§é©åˆ‡ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æä¾›ã•ã‚ŒãŸã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‚’å…ƒã«ã€ä»¥ä¸‹ã®æ‰‹é †ã‚’ã€Œä¸€æ­©ãšã¤ç®—æ•°ã¨ã—ã¦æ›¸ãå‡ºã—ã¦ã€é€æ–™ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘:
{context}

è³ªå•: {req.question}

ã€åˆ†æã‚¹ãƒ†ãƒƒãƒ—ã€‘:
1. **æ„å›³åˆ¤å®š**: è³ªå•ã¯ã€Œå˜ç´”ãªäº‹å®Ÿç¢ºèªï¼ˆä¾¡æ ¼ãƒ»ç‰¹å¾´ãªã©ï¼‰ã€ã‹ã€ãã‚Œã¨ã‚‚ã€Œé€æ–™ã®è¨ˆç®—ã€ãŒå¿…è¦ã‹ï¼Ÿ
2. **æƒ…å ±ã®å……è¶³æ€§**: 
   - äº‹å®Ÿç¢ºèªãªã‚‰ï¼šãƒ‡ãƒ¼ã‚¿ã«å›ç­”ãŒã‚ã‚‹ã‹ï¼Ÿ
   - é€æ–™è¨ˆç®—ãªã‚‰ï¼šé…é€å…ˆã¨æ•°é‡ãŒæƒã£ã¦ã„ã‚‹ã‹ï¼Ÿ

ã€å›ç­”ãƒ«ãƒ¼ãƒ«ã€‘:
- ä¾¡æ ¼ã‚„ç‰¹å¾´ã®ã¿ã‚’èã‹ã‚ŒãŸå ´åˆã¯ã€é€æ–™ã®è©±ã¯ä¸€åˆ‡ã›ãšã€ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå³ç­”ã—ã¦ãã ã•ã„ã€‚
- é€æ–™ã®è¨ˆç®—ãŒå¿…è¦ã§æƒ…å ±ãŒè¶³ã‚Šãªã„å ´åˆã®ã¿ã€ä¸è¶³åˆ†ï¼ˆåœ°åŸŸãƒ»æ•°ï¼‰ã‚’å„ªã—ãèãè¿”ã—ã¦ãã ã•ã„ã€‚
- å…¨ã¦æƒã£ã¦ã„ã‚‹å ´åˆã¯ã€ç®—æ•°ã¨ã—ã¦è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ›¸ãå‡ºã—ã¦ãã ã•ã„ã€‚

ã€é€æ–™è¨ˆç®—ã®é‰„å‰‡ã€‘:
1. **æƒ…å ±ä¸è¶³ã®ç¢ºèªï¼ˆé‡è¦ï¼‰**:
  - ã€Œã©ã“ã¸é€ã‚‹ã‹ï¼ˆåœ°åŸŸï¼‰ã€ãŒä¸æ˜ãªå ´åˆã€ã¾ãŸã¯ã€Œä½•ã‚’ã„ãã¤é€ã‚‹ã‹ï¼ˆå•†å“ã¨æ•°ï¼‰ã€ãŒä¸æ˜ãªå ´åˆã¯ã€è¨ˆç®—ã‚’ä¸­æ–­ã—ã€Œä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ã€ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
2. **å•†å“ã®ãƒã‚¤ãƒ³ãƒˆç‰¹å®š**: è³ªå•ã•ã‚ŒãŸå„å•†å“ã«ã¤ã„ã¦ã€ã€Œ1è¢‹ï¼ˆã¾ãŸã¯1ã‚»ãƒƒãƒˆï¼‰ã‚ãŸã‚Šã®ãƒã‚¤ãƒ³ãƒˆã€ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¦‹ã¤ã‘ã‚‹ã€‚
3. **åˆç®—å¼ã®ä½œæˆ**: ã€Œï¼ˆå•†å“Aã®ãƒã‚¤ãƒ³ãƒˆ Ã— æ³¨æ–‡è¢‹æ•°ï¼‰ + ï¼ˆå•†å“Bã®ãƒã‚¤ãƒ³ãƒˆ Ã— æ³¨æ–‡è¢‹æ•°ï¼‰...ã€ã¨ã„ã†å¼ã‚’ç«‹ã¦ã‚‹ã€‚
   - æ³¨æ„ï¼šãƒ†ã‚£ãƒ¼ãƒãƒƒã‚°ã®å€‹æ•°ï¼ˆä¾‹ï¼š8å€‹å…¥ã‚Šï¼‰ã§ã¯ãªãã€æ³¨æ–‡ã•ã‚ŒãŸã€Œè¢‹æ•°ã€ã‚’æ›ã‘ã‚‹ã“ã¨ã€‚
4. **åˆ¤å®š**: åˆè¨ˆãŒ **6.0ãƒã‚¤ãƒ³ãƒˆä»¥ä¸‹** ãªã‚‰ã€å…¨å›½ä¸€å¾‹ **280å††**ã€‚
   - åˆè¨ˆãŒ **6.0ãƒã‚¤ãƒ³ãƒˆã‚’è¶…ãˆã‚‹å ´åˆã®ã¿**ã€åœ°åŸŸã®é€æ–™ï¼ˆä¾‹ï¼šæ²–ç¸„çœŒï¼‰ã‚’è¡¨ã‹ã‚‰å¼•ç”¨ã™ã‚‹ã€‚
5. **é€æ–™ç„¡æ–™**: ç¨è¾¼åˆè¨ˆãŒ20,000å††ä»¥ä¸Šãªã‚‰0å††ã€‚

å›ç­”ï¼ˆè¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã‚’å…¨ã¦æ›¸ãã“ã¨ï¼‰:"""
    
    async with httpx.AsyncClient(timeout=None) as client:
        logic_resp = await client.post(
            f"{OLLAMA_URL}/api/generate",
            json={"model": MODEL_NAME, "prompt": logic_prompt, "stream": False, "options": {"temperature": 0.0}}
        )
        raw_answer = logic_resp.json().get("response")

        # ã‚¹ãƒ†ãƒƒãƒ—4: æ•´å½¢ãƒ»æ¤œè¨¼ï¼ˆæ¥å®¢ç”¨æ¸…æ›¸ï¼‰
        clean_up_prompt = f"""ã‚ãªãŸã¯è—¤å…«èŒ¶å¯®ã®çœ‹æ¿ã‚¹ã‚¿ãƒƒãƒ•ã€ŒèŒ¶ã€…ä¸¸ã€ã§ã™ã€‚
ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘ã‚’å…ƒã«ã€ãŠå®¢æ§˜ã®è³ªå•ã«ã ã‘ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã€‘:
{context}
ã€è¨ˆç®—ãƒ‰ãƒ©ãƒ•ãƒˆã€‘:
{raw_answer}

æ¸…æ›¸ãƒ«ãƒ¼ãƒ«:
1. **å›ç­”æœ¬æ–‡ã«ã¯è¨ˆç®—å¼ã‚„ãƒã‚¤ãƒ³ãƒˆã®æ•°å€¤ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã€‚**
2. çµè«–ï¼ˆé‡‘é¡ï¼‰ãŒãƒ‡ãƒ¼ã‚¿ã®ãƒ«ãƒ¼ãƒ«ã¨åˆã£ã¦ã„ã‚‹ã‹ã€ãƒ‰ãƒ©ãƒ•ãƒˆã‚’éä¿¡ã›ãšå†ç¢ºèªã™ã‚‹ã€‚
3. è³ªå•ã«é€æ–™ãŒå«ã¾ã‚Œãªã„å ´åˆã¯ã€é€æ–™ã®è©±ã¯ä¸€åˆ‡ã—ãªã„ã€‚
4. èŒ¶ã€…ä¸¸ã¨ã—ã¦æ¸©ã‹ã„ãƒˆãƒ¼ãƒ³ã§å›ç­”ã™ã‚‹ã€‚

å›ç­”ï¼ˆæ¥å®¢ç”¨æ¸…æ›¸ï¼‰:"""

        final_resp = await client.post(
            f"{OLLAMA_URL}/api/generate",
            json={"model": MODEL_NAME, "prompt": clean_up_prompt, "stream": False, "options": {"temperature": 0.0}}
        )
        
        return {
            "answer": final_resp.json().get("response"),
            "weights": weights,
            "debug_raw_logic": raw_answer  # æ¤œè¨¼ç”¨ã«æ®‹ã—ã¦ã„ã¾ã™
        }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
