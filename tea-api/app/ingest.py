import os
import json
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# --- è¨­å®š ---
OLLAMA_URL = "http://ollama:11434"
EMBED_MODEL = "gemma2:9b"
DB_DIR = "./db"
KNOWLEDGE_DIR = "./knowledge"

def main():
    documents = []
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¢ç´¢ã¨èª­ã¿è¾¼ã¿
    for root, dirs, files in os.walk(KNOWLEDGE_DIR):
        for file in files:
            path = os.path.join(root, file)
            # ãƒ•ã‚©ãƒ«ãƒ€åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
            doc_type = "short" if "short" in root else "medium"

            # --- JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç† ---
            if file.endswith(".json"):
                print(f"ğŸ“„ Processing JSON: {path}")
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                        # ãƒªã‚¹ãƒˆå½¢å¼ï¼ˆ[{å•†å“1}, {å•†å“2}]ï¼‰ã‚’æƒ³å®š
                        items = data if isinstance(data, list) else [data]
                        
                        for item in items:
                            # AIãŒæ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆã‚­ãƒ¼ï¼šå€¤ ã®å½¢å¼ï¼‰
                            content = "\n".join(f"{k}: {v}" for k, v in item.items())
                            
                            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚‚æƒ…å ±ã‚’ä¿æŒï¼ˆå°†æ¥ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
                            doc = Document(
                                page_content=content,
                                metadata={
                                    "source": path, 
                                    "doc_type": doc_type,
                                    **{k: str(v) for k, v in item.items()} # ã™ã¹ã¦æ–‡å­—åˆ—ã§ä¿æŒ
                                }
                            )
                            documents.append(doc)
                except Exception as e:
                    print(f"âŒ Error loading JSON {path}: {e}")

            # --- Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç† ---
            elif file.endswith(".md"):
                print(f"ğŸ“„ Processing Markdown: {path}")
                try:
                    loader = TextLoader(path)
                    docs = loader.load()
                    for d in docs:
                        d.metadata["doc_type"] = doc_type
                        d.metadata["source"] = path
                    documents.extend(docs)
                except Exception as e:
                    print(f"âŒ Error loading Markdown {path}: {e}")

    if not documents:
        print("âš ï¸ No documents found to ingest.")
        return

    # 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    # JSONãƒ‡ãƒ¼ã‚¿ã¯ã™ã§ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€Markdownã®ã¿ã‚’è€ƒæ…®ã—ãŸå¤§ãã‚ã®è¨­å®š
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000, 
        chunk_overlap=100,
        separators=["\n## ", "\n### ", "\n\n", "\n"]
    )
    split_docs = splitter.split_documents(documents)

    # 3. ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆ
    print(f"ğŸš€ Creating Vector DB with {len(split_docs)} chunks...")
    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model=EMBED_MODEL)
    
    # æ—¢å­˜ã®DBãŒã‚ã‚Œã°ä¸Šæ›¸ãã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆ
    vector_db = Chroma.from_documents(
        documents=split_docs, 
        embedding=embeddings, 
        persist_directory=DB_DIR
    )
    
    print(f"ğŸ‰ Success: Vector DB created at {DB_DIR}")

if __name__ == "__main__":
    main()